import requests
import json
import re
import random
import time
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException

# ë‹¤ì–‘í•œ User-Agent ë¦¬ìŠ¤íŠ¸ (ì•ˆí‹°-ë´‡ íšŒí”¼)
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/121.0"
]

def get_random_user_agent():
    """ëœë¤ User-Agent ë°˜í™˜"""
    return random.choice(USER_AGENTS)

def setup_chrome_options(proxy=None):
    """Chrome ë¸Œë¼ìš°ì € ì˜µì…˜ ì„¤ì • (í”„ë¡ì‹œ ì§€ì›)"""
    chrome_options = Options()
    
    # ê¸°ë³¸ ì˜µì…˜
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    
    # ëœë¤ User-Agent ì„¤ì •
    chrome_options.add_argument(f"--user-agent={get_random_user_agent()}")
    
    # í”„ë¡ì‹œ ì„¤ì • (ì„ íƒì‚¬í•­)
    if proxy:
        chrome_options.add_argument(f"--proxy-server={proxy}")
    
    # ì¶”ê°€ ì•ˆí‹°-ë´‡ íšŒí”¼ ì„¤ì •
    chrome_options.add_argument("--disable-extensions")
    chrome_options.add_argument("--disable-plugins-discovery")
    chrome_options.add_argument("--disable-web-security")
    chrome_options.add_argument("--allow-running-insecure-content")
    
    return chrome_options

def get_instagram_posts_advanced(instagram_url, proxy=None):
    """
    ê°œì„ ëœ ì¸ìŠ¤íƒ€ê·¸ë¨ í¬ìŠ¤íŠ¸ ìŠ¤í¬ë˜í•‘ (í”„ë¡ì‹œ ì§€ì›)
    
    Args:
        instagram_url (str): ì¸ìŠ¤íƒ€ê·¸ë¨ í”„ë¡œí•„ URL
        proxy (str): í”„ë¡ì‹œ ì„œë²„ (ì˜ˆ: "http://proxy:port")
        
    Returns:
        str: ìµœì‹  í¬ìŠ¤íŠ¸ì˜ í…ìŠ¤íŠ¸ ë‚´ìš©
    """
    driver = None
    try:
        # Chrome ì˜µì…˜ ì„¤ì •
        chrome_options = setup_chrome_options(proxy)
        driver = webdriver.Chrome(options=chrome_options)
        
        # ì•ˆí‹°-ë´‡ íšŒí”¼: navigator.webdriver ìˆ¨ê¸°ê¸°
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        # í˜ì´ì§€ ë¡œë“œ
        driver.get(instagram_url)
        
        # ëœë¤ ì§€ì—° (ë´‡ íƒì§€ íšŒí”¼)
        time.sleep(random.uniform(2, 5))
        
        # ì¿ í‚¤ ë°°ë„ˆ ë“± íŒì—… ì²˜ë¦¬
        try:
            # "ë‚˜ì¤‘ì— í•˜ê¸°" ë²„íŠ¼ í´ë¦­ (ë¡œê·¸ì¸ íŒì—…)
            later_button = WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.XPATH, "//button[contains(text(), 'ë‚˜ì¤‘ì—') or contains(text(), 'Not Now')]"))
            )
            later_button.click()
            time.sleep(1)
        except TimeoutException:
            pass  # íŒì—…ì´ ì—†ìœ¼ë©´ ê³„ì† ì§„í–‰
        
        # ì²« ë²ˆì§¸ í¬ìŠ¤íŠ¸ ì°¾ê¸° (ì—¬ëŸ¬ ì…€ë ‰í„° ì‹œë„)
        post_selectors = [
            "article div div div div a",
            "div[role='main'] article a",
            "main article a[role='link']",
            "div._ac7v a"  # Instagramì˜ ìƒˆë¡œìš´ í´ë˜ìŠ¤ëª…
        ]
        
        first_post = None
        for selector in post_selectors:
            try:
                first_post = WebDriverWait(driver, 10).until(
                    EC.element_to_be_clickable((By.CSS_SELECTOR, selector))
                )
                break
            except TimeoutException:
                continue
        
        if not first_post:
            raise Exception("ì²« ë²ˆì§¸ í¬ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # í¬ìŠ¤íŠ¸ í´ë¦­
        driver.execute_script("arguments[0].click();", first_post)
        time.sleep(random.uniform(2, 4))
        
        # í¬ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
        post_text = extract_post_text(driver)
        
        return post_text.strip()
        
    except Exception as e:
        print(f"ê³ ê¸‰ ì¸ìŠ¤íƒ€ê·¸ë¨ ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")
        return ""
        
    finally:
        if driver:
            driver.quit()

def extract_post_text(driver):
    """í¬ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)"""
    post_text = ""
    
    # ë°©ë²• 1: ìµœì‹  Instagram êµ¬ì¡°
    text_selectors = [
        "div[data-testid='post-caption'] span",
        "article div div div div span",
        "div._a9zs span",  # ìƒˆë¡œìš´ í´ë˜ìŠ¤ëª…
        "span[dir='auto']"
    ]
    
    for selector in text_selectors:
        try:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            for element in elements:
                text = element.text.strip()
                if text and len(text) > 10:
                    post_text += text + "\n"
            
            if post_text:
                break
                
        except Exception as e:
            continue
    
    # ë°©ë²• 2: í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
    if not post_text:
        try:
            all_spans = driver.find_elements(By.TAG_NAME, "span")
            menu_keywords = ['ë©”ë‰´', 'ì˜¤ëŠ˜', 'ìŒì‹', 'ìš”ë¦¬', 'ë·”í˜', 'í•œì‹', 'ë°˜ì°¬', 'êµ­', 'ì°Œê°œ']
            
            for element in all_spans:
                text = element.text.strip()
                if text and any(keyword in text for keyword in menu_keywords):
                    post_text += text + "\n"
                    
        except Exception as e:
            pass
    
    return post_text

def get_instagram_posts_requests(instagram_url, proxy=None):
    """
    requests + BeautifulSoupì„ ì‚¬ìš©í•œ ê°œì„ ëœ fallback ë°©ë²•
    """
    try:
        headers = {
            'User-Agent': get_random_user_agent(),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        # í”„ë¡ì‹œ ì„¤ì •
        proxies = None
        if proxy:
            proxies = {
                'http': proxy,
                'https': proxy
            }
        
        # ìš”ì²­ ì „ ëœë¤ ì§€ì—°
        time.sleep(random.uniform(1, 3))
        
        response = requests.get(
            instagram_url, 
            headers=headers, 
            proxies=proxies,
            timeout=15
        )
        
        if response.status_code != 200:
            return f"HTTP ì˜¤ë¥˜: {response.status_code}"
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ë°©ë²• 1: JSON-LD ë°ì´í„° ì¶”ì¶œ
        scripts = soup.find_all('script', type='application/ld+json')
        for script in scripts:
            try:
                data = json.loads(script.string)
                if isinstance(data, dict):
                    # ë‹¤ì–‘í•œ êµ¬ì¡° í™•ì¸
                    if 'mainEntity' in data and 'text' in data['mainEntity']:
                        return data['mainEntity']['text']
                    elif 'description' in data:
                        return data['description']
                        
            except json.JSONDecodeError:
                continue
        
        # ë°©ë²• 2: window._sharedData ì¶”ì¶œ
        script_tags = soup.find_all('script')
        for script in script_tags:
            if script.string and 'window._sharedData' in script.string:
                try:
                    # JSON ë°ì´í„° ì¶”ì¶œ ë¡œì§
                    json_match = re.search(r'window\._sharedData = ({.*?});', script.string)
                    if json_match:
                        shared_data = json.loads(json_match.group(1))
                        # í¬ìŠ¤íŠ¸ ë°ì´í„° íƒìƒ‰
                        return extract_from_shared_data(shared_data)
                except:
                    continue
        
        return "requests ë°©ì‹ìœ¼ë¡œ ë©”ë‰´ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
    except Exception as e:
        print(f"requests ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")
        return ""

def extract_from_shared_data(shared_data):
    """window._sharedDataì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        # Instagramì˜ ë³µì¡í•œ JSON êµ¬ì¡° íƒìƒ‰
        entry_data = shared_data.get('entry_data', {})
        profile_page = entry_data.get('ProfilePage', [])
        
        if profile_page:
            user_data = profile_page[0].get('graphql', {}).get('user', {})
            media_data = user_data.get('edge_owner_to_timeline_media', {}).get('edges', [])
            
            if media_data:
                latest_post = media_data[0].get('node', {})
                caption = latest_post.get('edge_media_to_caption', {}).get('edges', [])
                
                if caption:
                    return caption[0].get('node', {}).get('text', '')
        
        return ""
        
    except Exception as e:
        return ""

def scrape_menu_from_instagram(instagram_url, use_proxy=False, proxy=None):
    """
    ê°œì„ ëœ ì¸ìŠ¤íƒ€ê·¸ë¨ ë©”ë‰´ ìŠ¤í¬ë˜í•‘ ë©”ì¸ í•¨ìˆ˜
    
    Args:
        instagram_url (str): ì¸ìŠ¤íƒ€ê·¸ë¨ í”„ë¡œí•„ URL
        use_proxy (bool): í”„ë¡ì‹œ ì‚¬ìš© ì—¬ë¶€
        proxy (str): í”„ë¡ì‹œ ì„œë²„ ì£¼ì†Œ
    """
    print(f"ğŸ“± ì¸ìŠ¤íƒ€ê·¸ë¨ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {instagram_url}")
    
    if use_proxy and proxy:
        print(f"ğŸŒ í”„ë¡ì‹œ ì‚¬ìš©: {proxy}")
    
    # ë°©ë²• 1: ê³ ê¸‰ Selenium ìŠ¤í¬ë˜í•‘
    result = get_instagram_posts_advanced(instagram_url, proxy if use_proxy else None)
    
    # ë°©ë²• 2: requests + BeautifulSoup fallback
    if not result or len(result) < 20:
        print("ğŸ”„ Selenium ì‹¤íŒ¨, requests ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„...")
        result = get_instagram_posts_requests(instagram_url, proxy if use_proxy else None)
    
    # ë°©ë²• 3: ê¸°ì¡´ ë°©ì‹ fallback
    if not result or len(result) < 20:
        print("ğŸ”„ ëª¨ë“  ë°©ì‹ ì‹¤íŒ¨, ê¸°ë³¸ ìŠ¤í¬ë˜í¼ë¡œ ì¬ì‹œë„...")
        from .instagram_scraper_legacy import get_instagram_posts as legacy_scraper
        try:
            result = legacy_scraper(instagram_url)
        except:
            result = ""
    
    # ìµœì¢… fallback
    if not result or len(result) < 10:
        result = """
ğŸ½ï¸ êµ¬ë„ í•œì‹ë·”í˜ ë©”ë‰´ ì •ë³´
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âš ï¸ í˜„ì¬ ì¸ìŠ¤íƒ€ê·¸ë¨ì—ì„œ ë©”ë‰´ ì •ë³´ë¥¼ ìë™ìœ¼ë¡œ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ğŸ“± ì§ì ‘ í™•ì¸: https://www.instagram.com/sunaedong_buffet/

ğŸ“ ë§¤ì¥ ë¬¸ì˜: êµ¬ë„ í•œì‹ë·”í˜
ğŸ•’ ìš´ì˜ì‹œê°„: ì˜¤ì „ 11ì‹œ ~ ì˜¤í›„ 9ì‹œ

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        """.strip()
    
    print(f"âœ… ìŠ¤í¬ë˜í•‘ ì™„ë£Œ (ê¸¸ì´: {len(result)}ì)")
    return result

# ê¸°ì¡´ í•¨ìˆ˜ë“¤ (í•˜ìœ„ í˜¸í™˜ì„±)
def get_instagram_posts(instagram_url):
    """í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ í•¨ìˆ˜"""
    return scrape_menu_from_instagram(instagram_url)

def get_instagram_posts_fallback(instagram_url):
    """í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ í•¨ìˆ˜"""
    return get_instagram_posts_requests(instagram_url)

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    url = "https://www.instagram.com/sunaedong_buffet/"
    
    print("=== ê¸°ë³¸ ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸ ===")
    content = scrape_menu_from_instagram(url)
    print("ìŠ¤í¬ë˜í•‘ ê²°ê³¼:")
    print(content)
    
    print("\n=== í”„ë¡ì‹œ ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸ (ì£¼ì„ í•´ì œí•˜ì—¬ ì‚¬ìš©) ===")
    # proxy_server = "http://proxy-server:port"  # ì‹¤ì œ í”„ë¡ì‹œ ì£¼ì†Œ
    # content_with_proxy = scrape_menu_from_instagram(url, use_proxy=True, proxy=proxy_server)
    # print("í”„ë¡ì‹œ ìŠ¤í¬ë˜í•‘ ê²°ê³¼:")
    # print(content_with_proxy) 